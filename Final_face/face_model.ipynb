{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from net_sphere import sphere20a\n",
    "#refernece : https://github.com/davrempe/domain-transfer-net/blob/master/FaceMain.ipynb\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeTransform(object):\n",
    "    ''' Resizes a PIL image to (size, size) to feed into OpenFace net and returns a torch tensor.'''\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        img = sample.resize((self.size, self.size), Image.BILINEAR)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        return torch.from_numpy(img)\n",
    "\n",
    "\n",
    "class BitEmoji(Dataset):\n",
    "    '''\n",
    "    Dataset of 1 million bitmoji images.\n",
    "    start_idx - image number dataset should start at\n",
    "    end_idx - data number where dataset ends\n",
    "    '''\n",
    "    def __init__(self, data_dir, start_index=0, end_index=100000, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.data_len = end_index - start_index\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        \"\"\"\n",
    "        img_name = os.path.join(self.data_dir, 'emoji_{}.png'.format(idx))\n",
    "        img = Image.open(img_name)\n",
    "        img = img.convert('RGB') # as it's a png\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "                                   \n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len    \n",
    "    \n",
    "class MSCelebDataset(Dataset):\n",
    "    '''\n",
    "    MS-Celeb-1M face image dataset. This is the aligned and cropped version. \n",
    "    data_dir - directory of data. This directory should contain annotation files and a subdirectory for image data.\n",
    "    split - either 'train' or 'test'\n",
    "    '''\n",
    "    def __init__(self, data_dir, split, transform=None):\n",
    "        data_splits = ['train', 'test']\n",
    "        self.transform = transform\n",
    "        \n",
    "        split = data_splits.index(split)\n",
    "        if split == 0:\n",
    "            info_path = 'train_data_info.txt'\n",
    "            self.data_path = os.path.join(data_dir, 'images_train/')\n",
    "        elif split == 1:\n",
    "            info_path = 'info/test_data_info.txt'\n",
    "            self.data_path = os.path.join(data_dir, 'data/')\n",
    "        \n",
    "        info_data = []\n",
    "        with open(os.path.join(data_dir, info_path)) as info_file:\n",
    "            reader = csv.reader(info_file, delimiter=' ')\n",
    "            for row in reader:\n",
    "                info_data.append(row)\n",
    "                \n",
    "        self.info = np.array(info_data)\n",
    "        self.data_len = self.info.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        \"\"\"\n",
    "        img_name = os.path.join(self.data_path, self.info[idx, 0])\n",
    "        img = Image.open(img_name)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "                       \n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.target_path='/home/jupyter/emoji_data'\n",
    "# self.train_set_bitmoji = BitEmoji(a, 0, 100000, transform = ResizeTransform(96))\n",
    "# train_loader_bitmoji = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "# source_path = '/home/jupyter/MSceleb_data/data/'\n",
    "# train_set_celeb = MSCelebDataset(b, 'test', ResizeTransform(96))\n",
    "# train_loader_celeb = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.g = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(self.channels, 512, kernel_size=4, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(512),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    nn.Conv2d(512, 512, kernel_size=1, stride=1, padding=0),\n",
    "                    nn.BatchNorm2d(512),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "                    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(256),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0),\n",
    "                    nn.BatchNorm2d(256),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "                    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(128),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0),\n",
    "                    nn.BatchNorm2d(128),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "                    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    nn.Conv2d(64, 64, kernel_size=1, stride=1, padding=0),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "                    nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    nn.Conv2d(32, 32, kernel_size=1, stride=1, padding=0),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "                    nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
    "                    nn.Tanh())\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        resh_inp = inp.view(inp.size()[0], 512, 1, 1)\n",
    "        out = self.g(resh_inp)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class D(nn.Module):\n",
    "    def __init__(self, channels, al=0.2):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.al = al\n",
    "        self.d = nn.Sequential(\n",
    "                    nn.Conv2d(3, self.channels, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(self.channels),\n",
    "                    nn.LeakyReLU(self.al, inplace=True),\n",
    "        \n",
    "                    nn.Conv2d(self.channels, self.channels*2, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(self.channels*2),\n",
    "                    nn.LeakyReLU(self.al, inplace=True),\n",
    "        \n",
    "                    nn.Conv2d(self.channels*2, self.channels*4, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(self.channels*4),\n",
    "                    nn.LeakyReLU(self.al, inplace=True),\n",
    "        \n",
    "                    nn.Conv2d(self.channels*4, self.channels*2, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(self.channels*2),\n",
    "                    nn.LeakyReLU(self.al, inplace=True),\n",
    "        \n",
    "                    nn.Conv2d(self.channels*2, self.channels, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(self.channels),\n",
    "                    nn.LeakyReLU(self.al, inplace=True),\n",
    "                    \n",
    "                    nn.Conv2d(self.channels, 3, kernel_size=3, stride=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        out = self.d(inp)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = D(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ZeroPadBottom(object):\n",
    "    ''' Zero pads batch of image tensor Variables on bottom to given size. Input (B, C, H, W) - padded on H axis. '''\n",
    "    def __init__(self, size, use_gpu=True):\n",
    "        self.size = size\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        B, C, H, W = sample.size()\n",
    "        diff = self.size - H\n",
    "        padding = Variable(torch.zeros(B, C, diff, W), requires_grad=False)\n",
    "        if self.use_gpu:\n",
    "            padding = padding.cuda()\n",
    "        zero_padded = torch.cat((sample, padding), dim=2)\n",
    "        return zero_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_g = []\n",
    "l_d = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class tanh_unnormalize(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "  \n",
    "    def __call__(self, sample):\n",
    "        sample = sample * 0.5 + 0.5\n",
    "        return sample\n",
    "        \n",
    "class tanh_normalize(object):\n",
    "    ''' Normalizes a tensor with values from [0, 1] to [-1, 1]. '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        sample = sample * 2.0 - 1.0\n",
    "        return sample\n",
    "    \n",
    "class face_model():\n",
    "    def __init__(self, use_gpu=True):\n",
    "        super(face_model, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        self.generator_loss_func = None\n",
    "        self.discriminator_loss_func = None\n",
    "        self.gan_loss_func = None\n",
    "        self.generator_smooth_func = None\n",
    "        self.source_val_loader = None\n",
    "        self.source_test_loader = None\n",
    "        self.target_test_loader = None\n",
    "        self.source_train_loader = None\n",
    "        self.target_train_loader = None\n",
    "        self.batch_size = 128\n",
    "        self.lossCE = nn.CrossEntropyLoss()\n",
    "        self.target_path='/home/jupyter/emoji_data'\n",
    "        self.source_path ='/home/jupyter/MSceleb_data/'\n",
    "        \n",
    "\n",
    "    def make_loader(self):\n",
    "        '''TO DO'''\n",
    "        msface_transform = transforms.Compose(\n",
    "        [ResizeTransform(96),tanh_normalize()])\n",
    "        emoji_transform = transforms.Compose(\n",
    "        [ResizeTransform(96),tanh_normalize()])\n",
    "        source_train_set = MSCelebDataset(self.source_path, 'test',transform=msface_transform)\n",
    "        self.source_train_loader = torch.utils.data.DataLoader(source_train_set, batch_size=128,shuffle=True, num_workers=8)\n",
    "        \n",
    "        target_train_set = BitEmoji(self.target_path, 0,100000, transform = emoji_transform)\n",
    "        self.target_train_loader = torch.utils.data.DataLoader(target_train_set, batch_size=  128, shuffle=True, num_workers=8)\n",
    "        \n",
    "        \"\"\"\n",
    "            TEST SET CHUTIYA KAREGA\n",
    "        \"\"\"\n",
    "    \n",
    "    def make_model(self):\n",
    "        \n",
    "        self.model = {}\n",
    "        f = sphere20a(feature = True)\n",
    "        f.load_state_dict(torch.load('./sphere20a_20171020.pth'))\n",
    "        for params in f.parameters():\n",
    "            params.require_grad = False\n",
    "        \n",
    "        self.model['f'] = f\n",
    "        self.model['g'] = G(channels=512)\n",
    "        self.model['d'] = D(128)\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            self.model['g'] = self.model['g'].cuda()\n",
    "            self.model['d'] = self.model['d'].cuda()\n",
    "            self.model['f'] = self.model['f'].cuda()\n",
    "            \n",
    "        self.up = nn.Upsample(size=(96,96), mode='bilinear')\n",
    "        self.pad = ZeroPadBottom(112)\n",
    "        \n",
    "    def make_loss_func(self):\n",
    "        \n",
    "        self.lossCE = nn.CrossEntropyLoss().cuda()\n",
    "        self.lossMSE = nn.MSELoss().cuda()\n",
    "        lab0, lab1, lab2 = (torch.LongTensor(self.batch_size) for i in range(3))\n",
    "        lab0 = Variable(lab0.cuda())\n",
    "        lab1 = Variable(lab1.cuda())\n",
    "        lab2 = Variable(lab2.cuda())\n",
    "        \n",
    "        lab0.data.resize_(self.batch_size).fill_(0)\n",
    "        lab1.data.resize_(self.batch_size).fill_(1)\n",
    "        lab2.data.resize_(self.batch_size).fill_(2)\n",
    "        \n",
    "        self.lab0 = lab0\n",
    "        self.lab1 = lab1\n",
    "        self.lab2 = lab2\n",
    "        \n",
    "        self.make_generator_loss_func()\n",
    "        self.make_discriminator_loss_func()\n",
    "        self.make_smooth_func()\n",
    "        #self.make_dist_func_targ_domain()\n",
    "    \n",
    "    def make_opt(self):\n",
    "        \n",
    "        self.generator_opt = optim.Adam(self.model['g'].parameters(), lr = 2e-4, betas=(0.5, 0.999), weight_decay=1e-6)\n",
    "        self.discriminator_opt = optim.Adam(self.model['d'].parameters(), lr = 2e-4, betas=(0.5, 0.999), weight_decay=1e-6)\n",
    "        \n",
    "        self.generator_lr_sche = MultiStepLR(self.generator_opt, milestones=[15000], gamma=0.1)\n",
    "        self.discriminator_lr_sche = MultiStepLR(self.discriminator_opt, milestones=[15000], gamma=0.1)\n",
    "    \n",
    "    def cos_sim(self, x, y):\n",
    "        ab = torch.sum(x*y, dim=1)\n",
    "        a = torch.sqrt(torch.sum(x*x, dim=1))\n",
    "        b = torch.sqrt(torch.sum(y*y, dim=1))\n",
    "        sim = ab/(a*b)\n",
    "        avg_sim = torch.mean(sim)\n",
    "        cos_loss = 1-avg_sim\n",
    "        return cos_loss\n",
    "        \n",
    "    def make_smooth_func(self):\n",
    "        \n",
    "        def gen_smooth_func(s_g):\n",
    "            b,c,h,w = s_g.size()\n",
    "            \n",
    "            g_t = s_g.contiguous().view(b,c,h,w)\n",
    "            z_d = g_t[:,:,1:,:-1]\n",
    "            z_u = g_t[:,:,-1:,1:]\n",
    "            z = g_t[:,:,:-1,:-1]\n",
    "            \n",
    "            diff_sum = torch.abs(z_d-z) + torch.abs(z_u-z)\n",
    "            \n",
    "            loss = torch.mean(torch.sum(torch.sum(torch.mean(diff_sum,dim=1),dim=1),dim=1))\n",
    "            return loss\n",
    "        \n",
    "        self.gen_smooth_func = gen_smooth_func\n",
    "    \n",
    "    def make_generator_loss_func(self):\n",
    "        \n",
    "        def gloss(s_g, s_f, s_g_f, s_d_g, t, t_g, t_d_g, al=0.01, be=100, gam=0.0001):\n",
    "            l_gang = self.lossCE(s_d_g.squeeze(), self.lab2)+self.lossCE(t_d_g.squeeze(), self.lab2)\n",
    "            l_const = self.cos_sim(s_f.detach(), s_g_f)\n",
    "            ltv = self.gen_smooth_func(s_g)\n",
    "            ltid = self.lossMSE(t_g, t.detach())\n",
    "            \n",
    "            return l_gang + al*l_const + be*ltid + gam*ltv\n",
    "        \n",
    "        self.generator_loss_func = gloss\n",
    "        \n",
    "    def make_discriminator_loss_func(self):\n",
    "        \n",
    "        def dloss(s_d_g, t_d_g, t_d):\n",
    "            return self.lossCE(s_d_g.squeeze(), self.lab0) + self.lossCE(t_d_g.squeeze(), self.lab1) + self.lossCE(t_d.squeeze(), self.lab2)\n",
    "        \n",
    "        self.discriminator_loss_func = dloss\n",
    "    \n",
    "    def seeResultsSrc(self, s_data, s_G, t):     \n",
    "        s_data = s_data.cpu().data\n",
    "        s_G = s_G.cpu().data\n",
    "        t = t.cpu().data\n",
    "                \n",
    "        # Unnormalize images\n",
    "        unnorm_ms = tanh_unnormalize() #((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "        unnorm_emoji = tanh_unnormalize() #((0.2411, 0.1801, 0.1247), (0.3312, 0.2672, 0.2127))\n",
    "        unnorm_targ = tanh_unnormalize()\n",
    "        self.imshow(unnorm_ms(s_data[:16]))\n",
    "        self.imshow(unnorm_emoji(s_G[:16]))\n",
    "        self.imshow(unnorm_emoji(t[:16]))\n",
    "    \n",
    "    def imshow(self, img):\n",
    "        plt.figure()\n",
    "        npimg = torchvision.utils.make_grid(img, nrow=4).numpy()\n",
    "        npimg = np.transpose(npimg, (1, 2, 0)) \n",
    "        zero_array = np.zeros(npimg.shape)\n",
    "        one_array = np.ones(npimg.shape)\n",
    "        npimg = np.minimum(npimg,one_array)\n",
    "        npimg = np.maximum(npimg,zero_array)\n",
    "        \n",
    "        plt.imshow(npimg)\n",
    "        plt.show()\n",
    "    \n",
    "    def train(self, n_epo, **kwargs):\n",
    "        \n",
    "        l = min(len(self.source_train_loader), len(self.target_train_loader))\n",
    "        msimg_count = 0\n",
    "        tot_batch = 0\n",
    "        visualize_batches = 100 #kwargs.get(\"visualize_batches\", 50)\n",
    "        for e in range(n_epo):\n",
    "            print(\"Epoch\", e)\n",
    "            source_data_iter = iter(self.source_train_loader)\n",
    "            target_data_iter = iter(self.target_train_loader)\n",
    "            \n",
    "            for i in range(l):\n",
    "#                 print(i)\n",
    "                self.generator_lr_sche.step()\n",
    "                self.discriminator_lr_sche.step()\n",
    "                \n",
    "                msimg_count+=1\n",
    "                \n",
    "                if msimg_count >= len(self.source_train_loader):\n",
    "                    msimg_count = 0\n",
    "                    source_data_iter = iter(self.source_train_loader)\n",
    "                    \n",
    "                source_data = source_data_iter.next()\n",
    "                target_data = target_data_iter.next()\n",
    "                \n",
    "                if self.batch_size != source_data.size(0) or self.batch_size != target_data.size(0):\n",
    "                    continue\n",
    "                \n",
    "                tot_batch+=1\n",
    "                \n",
    "                if self.use_gpu:\n",
    "                    source_data = Variable(source_data.float().cuda())\n",
    "                    target_data = Variable(target_data.float().cuda())\n",
    "                else:\n",
    "                    source_data = Variable(source_data.float())\n",
    "                    target_data = Variable(target_data.float())\n",
    "                    \n",
    "                for param in self.model['d'].parameters():\n",
    "                    param.requires_grad = True\n",
    "                self.model['d'].zero_grad()\n",
    "                \n",
    "                source_data_pad = self.pad(source_data)\n",
    "                s_f = self.model['f'](source_data_pad)\n",
    "                s_g = self.model['g'](s_f)\n",
    "                s_g = self.up(s_g)\n",
    "#                 s_g_detach = s_g.detach()\n",
    "                s_d_g = self.model['d'](s_g)\n",
    "                \n",
    "                target_data_pad = self.pad(target_data)\n",
    "                t_f = self.model['f'](target_data_pad)\n",
    "                t_g = self.model['g'](t_f)\n",
    "                t_g = self.up(t_g)\n",
    "#                 t_g_detach = t_g.detach()\n",
    "                t_d_g = self.model['d'](t_g)\n",
    "                \n",
    "                t_d = self.model['d'](target_data)\n",
    "                \n",
    "                d_loss = self.discriminator_loss_func(s_d_g, t_d_g, t_d)\n",
    "                d_loss.backward()\n",
    "                self.discriminator_opt.step()\n",
    "                \n",
    "                for param in self.model['d'].parameters():\n",
    "                    param.requires_grad = False\n",
    "                self.model['g'].zero_grad()\n",
    "                \n",
    "                source_data_pad = self.pad(source_data)\n",
    "                s_f = self.model['f'](source_data_pad)\n",
    "                s_g = self.model['g'](s_f)\n",
    "                s_g = self.up(s_g)\n",
    "#                 s_g_detach = s_g.detach()\n",
    "                s_d_g = self.model['d'](s_g)\n",
    "                s_g_pad = self.pad(s_g)\n",
    "                s_g_f = self.model['f'](s_g_pad)\n",
    "                \n",
    "                target_data_pad = self.pad(target_data)\n",
    "                t_f = self.model['f'](target_data_pad)\n",
    "                t_g = self.model['g'](t_f)\n",
    "                t_g = self.up(t_g)\n",
    "#                 t_g_detach = t_g.detach()\n",
    "                t_d_g = self.model['d'](t_g)\n",
    "                \n",
    "                g_loss = self.generator_loss_func(s_g, s_f, s_g_f, s_d_g, target_data, t_g, t_d_g)\n",
    "                g_loss.backward()\n",
    "                self.generator_opt.step()\n",
    "                l_d.append(d_loss.data.item())\n",
    "                l_g.append(g_loss.data.item())\n",
    "\n",
    "                if i%visualize_batches == 0 :\n",
    "                    source_data_padded = self.pad(source_data)\n",
    "                    s_f = self.model['f'](source_data_padded)\n",
    "                    s_g = self.model['g'](s_f)\n",
    "                    # upscale\n",
    "                    s_g = self.up(s_g) \n",
    "                    self.seeResultsSrc(source_data, s_g, target_data)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = face_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.make_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.make_loss_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.make_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/jupyter/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/jupyter/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-2-97bce4a37eb1>\", line 30, in __getitem__\n    img = Image.open(img_name)\n  File \"/home/jupyter/.local/lib/python3.5/site-packages/PIL/Image.py\", line 2634, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/home/jupyter/emoji_data/emoji_85536.png'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-80c2766fe2ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-71f44ad22d9a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epo, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0msource_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_data_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mtarget_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_data_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msource_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Traceback (most recent call last):\n  File \"/home/jupyter/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/jupyter/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-2-97bce4a37eb1>\", line 30, in __getitem__\n    img = Image.open(img_name)\n  File \"/home/jupyter/.local/lib/python3.5/site-packages/PIL/Image.py\", line 2634, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/home/jupyter/emoji_data/emoji_85536.png'\n"
     ]
    }
   ],
   "source": [
    "model.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sphere20a(feature = True)\n",
    "f.load_state_dict(torch.load('./sphere20a_20171020.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sphere20a(\n",
       "  (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (relu1_1): PReLU(num_parameters=64)\n",
       "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1_2): PReLU(num_parameters=64)\n",
       "  (conv1_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1_3): PReLU(num_parameters=64)\n",
       "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (relu2_1): PReLU(num_parameters=128)\n",
       "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2_2): PReLU(num_parameters=128)\n",
       "  (conv2_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2_3): PReLU(num_parameters=128)\n",
       "  (conv2_4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2_4): PReLU(num_parameters=128)\n",
       "  (conv2_5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2_5): PReLU(num_parameters=128)\n",
       "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (relu3_1): PReLU(num_parameters=256)\n",
       "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3_2): PReLU(num_parameters=256)\n",
       "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3_3): PReLU(num_parameters=256)\n",
       "  (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3_4): PReLU(num_parameters=256)\n",
       "  (conv3_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3_5): PReLU(num_parameters=256)\n",
       "  (conv3_6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3_6): PReLU(num_parameters=256)\n",
       "  (conv3_7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3_7): PReLU(num_parameters=256)\n",
       "  (conv3_8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3_8): PReLU(num_parameters=256)\n",
       "  (conv3_9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3_9): PReLU(num_parameters=256)\n",
       "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (relu4_1): PReLU(num_parameters=512)\n",
       "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu4_2): PReLU(num_parameters=512)\n",
       "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu4_3): PReLU(num_parameters=512)\n",
       "  (fc5): Linear(in_features=21504, out_features=512, bias=True)\n",
       "  (fc6): AngleLinear()\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "gener = {'state_dict':OrderedDict()}\n",
    "gener['state_dict']=model.model['g'].state_dict\n",
    "disc = {'state_dict':OrderedDict()}\n",
    "disc['state_dict']=model.model['d'].state_dict\n",
    "torch.save(gener['state_dict'],'generator_al1e-2_be100_gam_1e-4.tar')\n",
    "torch.save(disc['state_dict'],'discriminator_al1e-2_be100_gam_1e-4.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
