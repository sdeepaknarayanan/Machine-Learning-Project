{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLFiRJBlmzfi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# TODO create a class for each dataset EXCEPT MNIST (this is already built into pytorch)\n",
    "# If the dataset comes pre-split into train/test we should write a separate class for each.\n",
    "\n",
    "# For example the street view house number might look something like this...\n",
    "# The cropped version of the dataset is in a weird *.mat format, see https://stackoverflow.com/questions/29185493/read-svhn-dataset-in-python for instructions to load with numpy\n",
    "class SVHNDataset(Dataset):\n",
    "    \"\"\"`SVHN <http://ufldl.stanford.edu/housenumbers/>`_ Dataset.\n",
    "    Note: The SVHN dataset assigns the label `10` to the digit `0`. However, in this Dataset,\n",
    "    we assign the label `0` to the digit `0` to be compatible with PyTorch loss functions which\n",
    "    expect the class labels to be in the range `[0, C-1]`\n",
    "    Args:\n",
    "        data_dir (string): directory of dataset where directory\n",
    "            ``SVHN`` exists.\n",
    "        split (string): One of {'train', 'test', 'extra'}.\n",
    "            Accordingly dataset is selected. 'extra' is Extra training set.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "    filename = \"\"\n",
    "    filepath = \"\"\n",
    "    split_list = {\n",
    "        'train': \"train_32x32.mat\",\n",
    "        'test': \"test_32x32.mat\",\n",
    "        'extra': \"extra_32x32.mat\"}\n",
    "\n",
    "    def __init__(self, data_dir='./datasets', split='train',\n",
    "                 transform=None, target_transform=None, download=False):\n",
    "        self.data_dir = os.path.expanduser(data_dir)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.split = split  # training set or test set or extra set\n",
    "\n",
    "        if self.split not in self.split_list:\n",
    "            raise ValueError('Wrong split entered! Please use split=\"train\" '\n",
    "                             'or split=\"extra\" or split=\"test\"')\n",
    "\n",
    "        self.filename = self.split_list[split]\n",
    "        self.filepath = os.path.join(self.data_dir, self.filename)\n",
    "        \n",
    "        # import here rather than at top of file because this is\n",
    "        # an optional dependency for torchvision\n",
    "        import scipy.io as sio\n",
    "        \n",
    "        \n",
    "        # judge if .mat exist\n",
    "        if not os.path.isfile(self.filepath):\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                ' You can use fetch_data.sh to download it')\n",
    "        \n",
    "        # reading(loading) mat file as array\n",
    "        loaded_mat = sio.loadmat(self.filepath)\n",
    "\n",
    "        self.data = loaded_mat['X']\n",
    "        # loading from the .mat file gives an np array of type np.uint8\n",
    "        # converting to np.int64, so that we have a LongTensor after\n",
    "        # the conversion from the numpy array\n",
    "        # the squeeze is needed to obtain a 1D tensor\n",
    "        self.labels = loaded_mat['y'].astype(np.int64).squeeze()\n",
    "\n",
    "        # the svhn dataset assigns the class label \"10\" to the digit 0\n",
    "        # this makes it inconsistent with several loss functions\n",
    "        # which expect the class labels to be in the range [0, C-1]\n",
    "        np.place(self.labels, self.labels == 10, 0)\n",
    "        self.data = np.transpose(self.data, (3, 2, 0, 1))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.labels[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(np.transpose(img, (1, 2, 0)))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "class EmojiDataset(Dataset):\n",
    "    '''\n",
    "    Dataset of 1 million bitmoji images.\n",
    "    start_idx - image number dataset should start at\n",
    "    end_idx - data number where dataset ends\n",
    "    '''\n",
    "    def __init__(self, data_dir, start_idx=0, end_idx=1000000, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.data_len = end_idx - start_idx\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        \"\"\"\n",
    "        img_name = os.path.join(self.data_dir, 'emoji_{}.png'.format(idx))\n",
    "        img = Image.open(img_name)\n",
    "        img = img.convert('RGB') # b/c it's a png\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "                                   \n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len    \n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    '''\n",
    "    CelebA face image dataset. This is the aligned and cropped version. \n",
    "    data_dir - directory of image data\n",
    "    ann_dir - directory of annotation data\n",
    "    split - either 'train', 'eval', or 'test'\n",
    "    '''\n",
    "    def __init__(self, data_dir, ann_dir, split, transform=None):\n",
    "                \n",
    "        data_splits = ['train', 'eval', 'test']\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        split = data_splits.index(split)\n",
    "        split_data = []\n",
    "        with open(os.path.join(ann_dir, 'list_eval_partition.txt')) as split_file:\n",
    "            reader = csv.reader(split_file, delimiter=' ')\n",
    "            for row in reader:\n",
    "                split_data.append(row)\n",
    "        bbox_data = []\n",
    "        with open(os.path.join(ann_dir, 'list_bbox_celeba.txt')) as bbox_file:\n",
    "            reader = csv.reader(bbox_file, delimiter=' ', skipinitialspace=True)\n",
    "            test_row = next(reader) # header row\n",
    "            test_row = next(reader) # header row\n",
    "            for row in reader:\n",
    "                bbox_data.append(row)\n",
    "                \n",
    "        split_data = np.array(split_data)\n",
    "        bbox_data = np.array(bbox_data)\n",
    "        split_inds = np.where(split_data[:,1] == str(split))[0]\n",
    "        \n",
    "        self.split_info = split_data[split_inds, :]\n",
    "        self.bbox_info = bbox_data[split_inds, :]\n",
    "        self.data_len = self.split_info.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        \"\"\"\n",
    "        img_name = os.path.join(self.data_dir, self.split_info[idx, 0])\n",
    "        img = Image.open(img_name)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "                           \n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "class MSCeleb1MDataset(Dataset):\n",
    "    '''\n",
    "    MS-Celeb-1M face image dataset. This is the aligned and cropped version. \n",
    "    data_dir - directory of data. This directory should contain annotation files and a subdirectory for image data.\n",
    "    split - either 'train' or 'test'\n",
    "    '''\n",
    "    def __init__(self, data_dir, split, transform=None):\n",
    "        data_splits = ['train', 'test']\n",
    "        self.transform = transform\n",
    "        \n",
    "        split = data_splits.index(split)\n",
    "        if split == 0:\n",
    "            info_path = 'train_data_info.txt'\n",
    "            self.data_path = os.path.join(data_dir, 'images_train/')\n",
    "        elif split == 1:\n",
    "            info_path = 'test_data_info.txt'\n",
    "            self.data_path = os.path.join(data_dir, 'images_test/')\n",
    "        \n",
    "        info_data = []\n",
    "        with open(os.path.join(data_dir, info_path)) as info_file:\n",
    "            reader = csv.reader(info_file, delimiter=' ')\n",
    "            for row in reader:\n",
    "                info_data.append(row)\n",
    "                \n",
    "        self.info = np.array(info_data)\n",
    "        self.data_len = self.info.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        \"\"\"\n",
    "        img_name = os.path.join(self.data_path, self.info[idx, 0])\n",
    "        img = Image.open(img_name)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "                       \n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "class ResizeTransform(object):\n",
    "    ''' Resizes a PIL image to (size, size) to feed into OpenFace net and returns a torch tensor.'''\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        img = sample.resize((self.size, self.size), Image.BILINEAR)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        return torch.from_numpy(img)\n",
    "    \n",
    "class ZeroPadBottom(object):\n",
    "    ''' Zero pads batch of image tensor Variables on bottom to given size. Input (B, C, H, W) - padded on H axis. '''\n",
    "    def __init__(self, size, use_gpu=True):\n",
    "        self.size = size\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        B, C, H, W = sample.size()\n",
    "        diff = self.size - H\n",
    "        padding = Variable(torch.zeros(B, C, diff, W), requires_grad=False)\n",
    "        if self.use_gpu:\n",
    "            padding = padding.cuda()\n",
    "        zero_padded = torch.cat((sample, padding), dim=2)\n",
    "        return zero_padded\n",
    "    \n",
    "class NormalizeRangeTanh(object):\n",
    "    ''' Normalizes a tensor with values from [0, 1] to [-1, 1]. '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        sample = sample * 2.0 - 1.0\n",
    "        return sample\n",
    "    \n",
    "class UnNormalizeRangeTanh(object):\n",
    "    ''' Unnormalizes a tensor with values from [-1, 1] to [0, 1]. '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        sample = (sample + 1.0) * 0.5\n",
    "        return sample\n",
    "        \n",
    "    \n",
    "class UnNormalize(object):\n",
    "    ''' from https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/3'''\n",
    "    def __init__(self, mean, std):\n",
    "        mean_arr = []\n",
    "        for dim in range(len(mean)):\n",
    "            mean_arr.append(dim)\n",
    "        std_arr = []\n",
    "        for dim in range(len(std)):\n",
    "            std_arr.append(dim)\n",
    "        self.mean = torch.Tensor(mean_arr).view(1, len(mean), 1, 1)\n",
    "        self.std = torch.Tensor(std_arr).view(1, len(std), 1, 1)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (B, C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        tensor *= self.std\n",
    "        tensor += self.mean\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urhdeKTklXx8"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aytVUIormZG5",
    "outputId": "68301ad7-c647-46a1-9228-86ab24e14b71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/extra_32x32.mat to ./SVHN/extra_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1327882240/1329278602 [01:52<00:00, 11148281.51it/s]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./SVHN/test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/64275384 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 16384/64275384 [00:00<08:52, 120758.78it/s]\u001b[A\n",
      "  0%|          | 40960/64275384 [00:00<07:59, 134089.40it/s]\u001b[A\n",
      "  0%|          | 65536/64275384 [00:00<07:22, 145231.19it/s]\u001b[A\n",
      "  0%|          | 98304/64275384 [00:00<06:29, 164874.52it/s]\u001b[A\n",
      "  0%|          | 147456/64275384 [00:00<05:25, 196975.25it/s]\u001b[A\n",
      "  0%|          | 196608/64275384 [00:01<04:41, 228014.04it/s]\u001b[A\n",
      "  0%|          | 253952/64275384 [00:01<04:02, 264456.74it/s]\u001b[A\n",
      "  0%|          | 319488/64275384 [00:01<03:29, 305006.45it/s]\u001b[A\n",
      "  1%|          | 393216/64275384 [00:01<03:01, 351933.95it/s]\u001b[A\n",
      "  1%|          | 466944/64275384 [00:01<02:42, 393281.49it/s]\u001b[A\n",
      "  1%|          | 565248/64275384 [00:01<02:19, 455376.65it/s]\u001b[A\n",
      "  1%|          | 679936/64275384 [00:01<02:00, 528141.03it/s]\u001b[A\n",
      "  1%|▏         | 811008/64275384 [00:02<01:43, 610951.04it/s]\u001b[A\n",
      "1329283072it [02:10, 11148281.51it/s]                                \n",
      "  2%|▏         | 1171456/64275384 [00:02<01:14, 844639.22it/s]\u001b[A\n",
      "  2%|▏         | 1409024/64275384 [00:02<01:02, 999201.35it/s]\u001b[A\n",
      "  3%|▎         | 1695744/64275384 [00:02<00:52, 1185818.92it/s]\u001b[A\n",
      "  3%|▎         | 2023424/64275384 [00:02<00:44, 1398816.23it/s]\u001b[A\n",
      "  4%|▎         | 2408448/64275384 [00:02<00:37, 1648801.33it/s]\u001b[A\n",
      "  4%|▍         | 2859008/64275384 [00:02<00:31, 1940802.96it/s]\u001b[A\n",
      "  5%|▌         | 3391488/64275384 [00:03<00:26, 2286437.86it/s]\u001b[A\n",
      "  6%|▋         | 4030464/64275384 [00:03<00:22, 2701447.75it/s]\u001b[A\n",
      "  7%|▋         | 4808704/64275384 [00:03<00:18, 3208329.04it/s]\u001b[A\n",
      "  9%|▉         | 5734400/64275384 [00:03<00:15, 3812437.67it/s]\u001b[A\n",
      " 11%|█         | 6864896/64275384 [00:03<00:12, 4550000.83it/s]\u001b[A\n",
      " 13%|█▎        | 8232960/64275384 [00:03<00:10, 5443654.57it/s]\u001b[A\n",
      " 15%|█▌        | 9895936/64275384 [00:03<00:08, 6529434.78it/s]\u001b[A\n",
      " 19%|█▊        | 11935744/64275384 [00:04<00:06, 7858537.13it/s]\u001b[A\n",
      " 22%|██▏       | 14368768/64275384 [00:04<00:05, 9447512.91it/s]\u001b[A\n",
      " 27%|██▋       | 17129472/64275384 [00:04<00:04, 11250236.94it/s]\u001b[A\n",
      " 31%|███       | 20070400/64275384 [00:04<00:03, 13065969.72it/s]\u001b[A\n",
      " 36%|███▌      | 22953984/64275384 [00:04<00:02, 14722115.41it/s]\u001b[A\n",
      " 40%|████      | 25829376/64275384 [00:04<00:02, 16134688.89it/s]\u001b[A\n",
      " 45%|████▍     | 28827648/64275384 [00:04<00:02, 17458681.40it/s]\u001b[A\n",
      " 49%|████▉     | 31670272/64275384 [00:05<00:01, 18291408.13it/s]\u001b[A\n",
      " 54%|█████▍    | 34619392/64275384 [00:05<00:01, 19098211.75it/s]\u001b[A\n",
      " 58%|█████▊    | 37552128/64275384 [00:05<00:01, 19648190.37it/s]\u001b[A\n",
      " 63%|██████▎   | 40624128/64275384 [00:05<00:01, 20441831.89it/s]\u001b[A\n",
      " 68%|██████▊   | 43769856/64275384 [00:05<00:00, 21077337.90it/s]\u001b[A\n",
      " 72%|███████▏  | 46579712/64275384 [00:05<00:00, 20943651.11it/s]\u001b[A\n",
      " 77%|███████▋  | 49283072/64275384 [00:05<00:00, 20492470.32it/s]\u001b[A\n",
      " 81%|████████▏ | 52330496/64275384 [00:06<00:00, 21013698.42it/s]\u001b[A\n",
      " 86%|████████▌ | 55377920/64275384 [00:06<00:00, 21134126.27it/s]\u001b[A\n",
      " 91%|█████████ | 58392576/64275384 [00:06<00:00, 21297336.96it/s]\u001b[A\n",
      " 96%|█████████▌| 61423616/64275384 [00:06<00:00, 21429026.29it/s]\u001b[A\n",
      "64282624it [00:22, 21429026.29it/s]                              \u001b[A"
     ]
    }
   ],
   "source": [
    "SVHN_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "s_train_set = torchvision.datasets.SVHN(root = './SVHN/', split='extra',download = True, transform = SVHN_transform)\n",
    "s_train_loader = torch.utils.data.DataLoader(s_train_set, batch_size=128,\n",
    "                                          shuffle=True, num_workers=8)\n",
    "s_test_set = torchvision.datasets.SVHN(root = './SVHN/', split='test', download = True, transform = SVHN_transform)\n",
    "s_test_loader = torch.utils.data.DataLoader(s_test_set, batch_size=128,\n",
    "                                         shuffle=False, num_workers=8)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qqM36WzUmhKg",
    "outputId": "c0de0be9-d8e8-432b-9333-f08735f3826c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import keras\n",
    "#from torchsummary import summary\n",
    "import torch\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wi1_LKwDnWNf"
   },
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "class F_me(nn.Module):\n",
    "\t'''\n",
    "\tMNIST digit classifier.\n",
    "\t'''\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(F_me, self).__init__()\n",
    "\t\tself.use_gpu = True\n",
    "\t\tself.classify = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),               \n",
    "                nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(256, 128, kernel_size=4, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(4),\n",
    "                Reshape(-1,128),\n",
    "                nn.Linear(128, 10),\n",
    "                nn.Softmax(),\n",
    "              )\n",
    "\t\tif self.use_gpu:        \n",
    "\t\t\tself.type(torch.cuda.FloatTensor)\n",
    "\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\t# TODO implement the forward pass\n",
    "\t\treturn self.classify(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "colab_type": "code",
    "id": "qoxa4Jb2naPw",
    "outputId": "76a0cd17-1e5d-4c6d-b97b-ff001c76e026"
   },
   "outputs": [],
   "source": [
    "model = F_me()\n",
    "#summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "colab_type": "code",
    "id": "3lFoZGpbtkMl",
    "outputId": "82de9804-ef5d-49ed-ab3e-bac0dce98f1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.5/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.840\n",
      "[1,  4000] loss: 1.680\n",
      "[2,  2000] loss: 1.622\n",
      "[2,  4000] loss: 1.540\n",
      "[3,  2000] loss: 1.498\n",
      "[3,  4000] loss: 1.496\n",
      "[4,  2000] loss: 1.493\n",
      "[4,  4000] loss: 1.492\n",
      "[5,  2000] loss: 1.490\n",
      "[5,  4000] loss: 1.490\n",
      "[6,  2000] loss: 1.489\n",
      "[6,  4000] loss: 1.488\n",
      "[7,  2000] loss: 1.486\n",
      "[7,  4000] loss: 1.488\n",
      "[8,  2000] loss: 1.486\n",
      "[8,  4000] loss: 1.487\n",
      "[9,  2000] loss: 1.486\n",
      "[9,  4000] loss: 1.486\n",
      "[10,  2000] loss: 1.485\n",
      "[10,  4000] loss: 1.485\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters())\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(s_train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "UWcpFD8rty-J",
    "outputId": "79445f39-ac83-4b57-ec6b-eda876a9c176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.29671173939767\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in s_test_loader:\n",
    "        images, labels = data\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print( (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pQqs08bdJQB-"
   },
   "outputs": [],
   "source": [
    "torch.save({'state_dict': model.state_dict()},'f_model.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "colab_type": "code",
    "id": "Zsqf-koMsuRk",
    "outputId": "46a58856-3dfd-4ffb-b6de-34d97b45c098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           1,792\n",
      "              ReLU-2           [-1, 64, 16, 16]               0\n",
      "            Conv2d-3            [-1, 128, 8, 8]          73,856\n",
      "              ReLU-4            [-1, 128, 8, 8]               0\n",
      "            Conv2d-5            [-1, 256, 4, 4]         295,168\n",
      "              ReLU-6            [-1, 256, 4, 4]               0\n",
      "            Conv2d-7            [-1, 128, 1, 1]         524,416\n",
      "              ReLU-8            [-1, 128, 1, 1]               0\n",
      "           Reshape-9                  [-1, 128]               0\n",
      "           Linear-10                   [-1, 10]           1,290\n",
      "          Softmax-11                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 896,522\n",
      "Trainable params: 896,522\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.44\n",
      "Params size (MB): 3.42\n",
      "Estimated Total Size (MB): 3.87\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "class F(nn.Module):\n",
    "\t'''\n",
    "\tMNIST digit classifier.\n",
    "\t'''\n",
    "\tdef __init__(self,):\n",
    "\t\tsuper(F, self).__init__()\n",
    "\t\tself.use_gpu = True\n",
    "\t\tself.classify = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                #nn.LeakyReLU(0.2, inplace=True),\n",
    "                \n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                #nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "                nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                #nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "                nn.Conv2d(256, 128, kernel_size=4, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                #nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "#                 Flatten(),\n",
    "                #nn.Linear(128, 10)\n",
    "                #nn.Conv2d(128, 10, kernel_size=1, stride=1, padding=0)\n",
    "                Reshape(-1,128),\n",
    "                nn.Linear(128, 10),\n",
    "                nn.Softmax()\n",
    "              )\n",
    "\t\tif self.use_gpu:        \n",
    "\t\t\tself.type(torch.cuda.FloatTensor)\n",
    "\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\t# TODO implement the forward pass\n",
    "\t\treturn self.classify(input)\n",
    "  \n",
    "model2 = F()\n",
    "summary(model2, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5p-V_SXJOg8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "id": "rF8vSFalnkoA",
    "outputId": "2ef767c9-f30b-47c9-981d-c081a3449e17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.902\n",
      "[1,  4000] loss: 1.651\n",
      "[2,  2000] loss: 1.602\n",
      "[2,  4000] loss: 1.594\n",
      "[3,  2000] loss: 1.587\n",
      "[3,  4000] loss: 1.584\n",
      "[4,  2000] loss: 1.580\n",
      "[4,  4000] loss: 1.579\n",
      "[5,  2000] loss: 1.576\n",
      "[5,  4000] loss: 1.575\n",
      "[6,  2000] loss: 1.573\n",
      "[6,  4000] loss: 1.573\n",
      "[7,  2000] loss: 1.562\n",
      "[7,  4000] loss: 1.499\n",
      "[8,  2000] loss: 1.496\n",
      "[8,  4000] loss: 1.495\n",
      "[9,  2000] loss: 1.493\n",
      "[9,  4000] loss: 1.493\n",
      "[10,  2000] loss: 1.491\n",
      "[10,  4000] loss: 1.492\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model2.parameters())\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(s_train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "4F7TUIRSn5bb",
    "outputId": "745ed120-f261-4753-a618-13e8bbfe36ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 91 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in s_test_loader:\n",
    "        images, labels = data\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = model2(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "pytorch-girish.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
