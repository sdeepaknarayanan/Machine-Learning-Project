{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4NJ2c5hiuyKS"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "#from base_test import BaseTest\n",
    "#import digits_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import data\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "#from data import NormalizeRangeTanh, UnNormalizeRangeTanh - What the hell is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwwe0MZuu53S"
   },
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, gpu=False): # What to do with GPU?\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.channels=channels\n",
    "        self.gpu=gpu\n",
    "        self.network=nn.Sequential(\n",
    "                     nn.ConvTranspose2d(self.channels,512,kernel_size=(4,4), stride=1, padding=1),\n",
    "                     nn.BatchNorm1d(512),\n",
    "                     nn.ReLU(inplace=True),\n",
    "                     nn.ConvTranspose2d(512,256,kernel_size=(4,4), stride=2, padding=1),\n",
    "                     nn.BatchNorm1d(256),\n",
    "                     nn.ReLU(inplace=True),\n",
    "                     nn.ConvTranspose2d(256,128,kernel_size=(4,4), stride=2, padding=1),\n",
    "                     nn.BatchNorm1d(128),\n",
    "                     nn.ReLU(inplace=True),\n",
    "                     nn.ConvTranspose2d(128,1,kernel_size=(4,4), stride=2, padding=1),\n",
    "                     nn.Tanh()\n",
    "        )\n",
    "    def forward(self,input):\n",
    "        return self.network(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1T6YDsRIxSP4"
   },
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "  \n",
    "    def __init__(self, channels, leaky_coef=0.2):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.channels=channels\n",
    "        self.leaky_coef=leaky_coef\n",
    "        self.network=nn.Sequential(\n",
    "                     nn.Conv2d(1,128,kernel_size=(3,3), stride=2, padding=1),\n",
    "                     nn.BatchNorm2d(128),\n",
    "                     nn.LeakyReLU(self.leaky_coef,inplace=True),\n",
    "                     nn.Conv2d(128,256,kernel_size=(3,3), stride=2, padding=1),\n",
    "                     nn.BatchNorm2d(256),\n",
    "                     nn.LeakyReLU(self.leaky_coef,inplace=True),\n",
    "                     nn.Conv2d(256,512,kernel_size=(3,3), stride=2, padding=1),\n",
    "                     nn.BatchNorm2d(512),\n",
    "                     nn.LeakyReLU(self.leaky_coef,inplace=True),\n",
    "                     nn.Conv2d(512,3, kernel_size=(4,4), stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self,input):\n",
    "        return self.network(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxatV29Sx5W9"
   },
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "\n",
    "class feature_encoder(nn.Module):\n",
    "    def __init__(self, channels, gpu=False):\n",
    "        super(feature_encoder, self).__init__()\n",
    "        self.gpu = gpu\n",
    "        self.channels = channels\n",
    "        self.classify = nn.Sequential(\n",
    "                       nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.MaxPool2d(2),               \n",
    "                    nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.Conv2d(256, 128, kernel_size=4, padding=0))\n",
    "#                     nn.ReLU(inplace=True),\n",
    "#                     nn.MaxPool2d(4),\n",
    "#                     Reshape(-1,128),\n",
    "#                     nn.Linear(128, 10),\n",
    "#                     nn.Softmax(),)\n",
    "        if self.gpu:\n",
    "            self.type(torch.cuda.FloatTensor)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.classify(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9aW0DY2zRLW"
   },
   "outputs": [],
   "source": [
    "# def classifier(address):\n",
    "#    # Kagi will guide us\n",
    "#   pass\n",
    "\n",
    "class tanh_normalize(object):\n",
    "    ''' Normalizes a tensor with values from [0, 1] to [-1, 1]. '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        sample = sample * 2.0 - 1.0\n",
    "        return sample\n",
    "\n",
    "class digits_transfer():\n",
    "  \n",
    "  \n",
    "    def __init__(self,gpu=True):\n",
    "    \n",
    "        super(digits_transfer,self).__init__()\n",
    "        self.gpu = gpu\n",
    "        self.generator_loss_function=None\n",
    "        self.gan_loss_function=None\n",
    "        self.discriminator_loss_function=None\n",
    "        #self.source_validation_loader=None\n",
    "        self.source_test_loader=None\n",
    "        self.target_test_loader=None\n",
    "        self.distance_target_domain=None\n",
    "        self.source_train_loader=None\n",
    "        self.target_train_loader=None\n",
    "        self.batch_size=128\n",
    "        self.lossCrossEntropy=nn.CrossEntropyLoss() ## Why the fuck is it here?\n",
    "    \n",
    "    def readClassifier(self, model_name):\n",
    "\n",
    "    #     old_model = torch.load(model_name)['state_dict']\n",
    "    #     old_dict = old_model.load_state_dict() \n",
    "    #     new_model = feature_encoder(3,self.gpu)\n",
    "    #     new_dict = new_model.state_dict()\n",
    "    #     new_dict = {k: v for k, v in old_dict.items() if k in new_dict}\n",
    "    #     old_dict.update(new_dict) \n",
    "    #     new_model.load_state_dict(new_dict)\n",
    "\n",
    "    #     model = describe_model()\n",
    "    #     checkpoint = torch.load('checkpoint.pth.tar')\n",
    "    #     model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        model = feature_encoder(3,self.gpu)\n",
    "        temp = torch.load(model_name)\n",
    "        model.load_state_dict(temp['state_dict'])\n",
    "        self.model['f'] =model\n",
    "\n",
    "        for param in self.model['f'].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def data_loader(self):\n",
    "\n",
    "        SVHN = transforms.Compose([transforms.ToTensor(), tanh_normalize()])\n",
    "        MNIST = transforms.Compose([transforms.Scale(32), transforms.ToTensor(), tanh_normalize()])\n",
    "\n",
    "        source_train_set = torchvision.datasets.SVHN(root='./SVHN/', split='extra', download=True, transform=SVHN)\n",
    "        self.source_train_loader = torch.utils.data.DataLoader(source_train_set, batch_size=128, shuffle=True, num_workers=8)\n",
    "\n",
    "        source_test_set = torchvision.datasets.SVHN(root='./SVHN/', split='test', download=True, transform=SVHN)\n",
    "        self.source_test_loader = torch.utils.data.DataLoader(source_test_set, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "        target_train_set = torchvision.datasets.MNIST(root='./MNIST/', train=True, download=True, transform=MNIST)\n",
    "        self.target_train_loader = torch.utils.data.DataLoader(target_train_set, batch_size=128, shuffle=True, num_workers=8)\n",
    "\n",
    "        target_test_set = torchvision.datasets.MNIST(root='./MNIST/', train=False, download=True, transform=MNIST)\n",
    "        self.target_test_loader = torch.utils.data.DataLoader(target_test_set, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "    def view_batch(self):\n",
    "    ## TO be filled in later\n",
    "        pass\n",
    "  \n",
    "    def make_model(self):\n",
    "\n",
    "        self.model = {}\n",
    "        self.model['d'] = discriminator(128)\n",
    "        self.model['g'] = generator(128)\n",
    "        if self.gpu:\n",
    "          self.model['d'] = self.model['d'].cuda()\n",
    "          self.model['g'] = self.model['g'].cuda()\n",
    "        # Assumption 1 : Kagi saves everything as a Tar File\n",
    "        #self.model['f'] = classifier('Give the saved model here')\n",
    "        self.readClassifier('final_f.tar')\n",
    "\n",
    "    def create_discriminator_loss_function(self):\n",
    "\n",
    "        def discriminator_loss(s_d_g, t_d_g, t_d):  \n",
    "            return self.lossCrossEntropy(s_d_g.squeeze(), self.label_0) + self.lossCrossEntropy(t_d_g, self.label_1) + self.lossCrossEntropy(t_d, self.label_2)\n",
    "\n",
    "        self.discriminator_loss_function = discriminator_loss\n",
    "  \n",
    "    def create_generator_loss_function(self):\n",
    "\n",
    "        def generator_loss(s_f, s_g_f, s_d_g, t, t_g, t_d_g, alpha, beta, gamma):\n",
    "\n",
    "            loss_gan_generator = self.lossCrossEntropy(s_d_g.squeeze, self.label_2) + self.lossCrossEntropy(t_d_g, self.label_2)\n",
    "            loss_feature_constancy = self.mean_square_loss(s_g_f, s_f.detach())\n",
    "            loss_target_identity = self.mean_square_loss(t_g, t.detach())\n",
    "\n",
    "            return loss_gan_generator + alpha*loss_feature_constancy + beta*loss_target_identity\n",
    "\n",
    "        self.generator_loss_function = generator_loss\n",
    "  \n",
    "\n",
    "    def make_loss_function(self):\n",
    "        self.lossCrossEntropy = nn.CrossEntropyLoss().cuda()\n",
    "        self.mean_square_loss = nn.MSELoss().cuda()\n",
    "        label_0, label_1, label_2 = (torch.LongTensor(self.batch_size) for i in range(3))\n",
    "        label_0 = Variable(label_0.cuda())\n",
    "        label_1 = Variable(label_1.cuda())\n",
    "        label_2 = Variable(label_2.cuda())\n",
    "        label_0.data.resize_(self.batch_size).fill_(0)\n",
    "        label_1.data.resize_(self.batch_size).fill_(1)\n",
    "        label_2.data.resize_(self.batch_size).fill_(2)\n",
    "        self.label_0 = label_0\n",
    "        self.label_1 = label_1\n",
    "        self.label_2 = label_2\n",
    "\n",
    "    #     self.create_distance_function_target_domain()\n",
    "        self.create_discriminator_loss_function()\n",
    "        self.create_generator_loss_function()\n",
    "  \n",
    "    def make_optimizer(self):\n",
    "    \n",
    "        generator_learning_rate = 0.001\n",
    "        generator_weight_decay = 0.000001     \n",
    "        self.generator_optimizer = optim.Adam(self.model['g'].parameters(), lr = generator_learning_rate, weight_decay = generator_weight_decay)\n",
    "\n",
    "        discriminator_learning_rate = 0.001\n",
    "        discriminator_weight_decay = 0.000001     \n",
    "        self.discriminator_optimizer = optim.Adam(self.model['d'].parameters(), lr = discriminator_learning_rate, weight_decay = discriminator_weight_decay)\n",
    "\n",
    "    #   def validate_model(self, **kwargs):\n",
    "\n",
    "    #     gan_loss_weight = kwargs.get(\"gan_loss_weight\",0.001)\n",
    "    #     validation_loss = 0\n",
    "    #     self.model['g'].eval()\n",
    "    #     samples = np.random.randint(0,len(source_validation_set),size = 5)\n",
    "    #     for i in samples:\n",
    "    # #       ## source_validation_set is a keyword argument!\n",
    "    # #       source_datLLLa = source_validation_set[i]\n",
    "    # #       source_generator_output = self.model['g'](source_data)\n",
    "    # #       source_classifier_output = self.model['f'](source_data)\n",
    "    # #       source_\n",
    "    # #     \"\"\"\n",
    "    # #       To be Done Later\n",
    "    # #     \"\"\"\n",
    "    # #     pass\n",
    "\n",
    "    def show_image():\n",
    "        pass\n",
    "        # Fuck this shit\n",
    "    def display_results(self, source_data, source_generator):\n",
    "        \"\"\"\n",
    "          To be done later\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def model_train(self, number_of_epochs, **kwargs):\n",
    "    \n",
    "        visualize_batch = kwargs.get(\"visualize_batch\",50)\n",
    "        save_batch = kwargs.get(\"save_batch\",200)\n",
    "        test_batch = kwargs.get(\"test_batch\",200)\n",
    "\n",
    "        SVHN_count=0\n",
    "        batch_count=0\n",
    "        discriminator_run_loss=0\n",
    "        generator_run_loss=0\n",
    "\n",
    "        l=min(len(self.source_train_loader),len(self.target_train_loader))\n",
    "\n",
    "        for epoch in range(number_of_epochs):\n",
    "\n",
    "            source_data_iter=iter(self.source_train_loader)\n",
    "            target_data_iter=iter(self.target_train_loader)\n",
    "\n",
    "            for i in range(l):\n",
    "\n",
    "                SVHN_count+=1\n",
    "                if SVHN_count>=len(self.source_train_loader):\n",
    "                    SVHN_count=0\n",
    "                    source_data_iter=iter(self.source_train_loader)\n",
    "\n",
    "                source_data, source_labels=source_data_iter.next()\n",
    "                target_data, target_labels=source_data_iter.next()\n",
    "\n",
    "                if self.batch_size != source_data.size(0) or self.batch_size != target_data.size(0):\n",
    "                    continue\n",
    "\n",
    "                batch_count+=1\n",
    "\n",
    "                if self.gpu:\n",
    "                    source_data, source_labels = Variable(source_data.float().cuda()), Variable(source_labels.float().cuda())\n",
    "                    target_data, target_labels = Variable(target_data.float().cuda()), Variable(target_labels.float().cuda())\n",
    "\n",
    "                else:\n",
    "                    source_data, source_labels = Variable(source_data.float()), Variable(source_labels.float())\n",
    "                    target_data, target_labels = Variable(target_data.float()), Variable(target_labels.float())\n",
    "\n",
    "                for param in self.model['d'].parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "                self.model['d'].zero_grad()\n",
    "\n",
    "                s_f = self.model['f'](source_data)      #generator \n",
    "                print(s_f.shape)\n",
    "                s_g = self.model['g'](s_f)\n",
    "                s_g_detach = s_g.detach()\n",
    "                s_d_g = self.model['d'](s_g_detach)\n",
    "\n",
    "                t_data_3 = torch.cat((t_data,t_data,t_data),1)\n",
    "                t_f = self.model['f'](t_data_3)\n",
    "                t_g = self.model['g'](t_f)\n",
    "                t_g_detach = t_g.detach()\n",
    "                t_d_g = self.model['d'](t_g_detach)\n",
    "                t_d = self.model['d'](t_data)\n",
    "\n",
    "                Discriminator_loss = self.discriminator_loss_function(s_d_g, t_d_g, t_d)\n",
    "                Discriminator_loss.backward()\n",
    "                self.discriminator_optimizer.step()\n",
    "\n",
    "                for param in self.model['g'].parameters():\n",
    "                      param.requires_grad = False\n",
    "\n",
    "                self.model['g'].zero_grad()\n",
    "\n",
    "                s_d_g = self.model['d'](s_g)\n",
    "                s_g_3 = torch.cat((s_g,s_g,s_g),1)\n",
    "                s_g_f = self.model['f'](s_g_3)\n",
    "\n",
    "                t_d_g = self.model['d'](t_g)\n",
    "\n",
    "                Generator_loss = self.generator_loss_function(s_f, s_g_f, s_d_g, t, t_g, t_d_g)\n",
    "                Generator_loss.backward()\n",
    "                self.generator_optimizer.step()\n",
    "\n",
    "                discriminator_run_loss+=Discriminator_loss.data[0]\n",
    "                generator_run_loss+=Generator_loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6q-Gq_1PCNaG"
   },
   "outputs": [],
   "source": [
    "dig=digits_transfer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-V_1A3Ag7k5u"
   },
   "outputs": [],
   "source": [
    "dig.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.load('f_model.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "new_dict = OrderedDict()\n",
    "final_dict = {'state_dict':new_dict}\n",
    "\n",
    "for elem in m['state_dict']:\n",
    "    #print(elem,(m['state_dict'][elem].shape))\n",
    "    if '13' not in elem:\n",
    "        final_dict['state_dict'][elem] = m['state_dict'][elem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_dict, 'final_f.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(m['state_dict']))a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "K5g4-2Px7_3-",
    "outputId": "8cf6e94b-2d00-40b7-a0f2-37446578b79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./SVHN/extra_32x32.mat\n",
      "Using downloaded and verified file: ./SVHN/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "dig.data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8KSGBxxC8UOv"
   },
   "outputs": [],
   "source": [
    "dig.make_optimizer()\n",
    "dig.make_loss_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1017
    },
    "colab_type": "code",
    "id": "0-7aNN979J59",
    "outputId": "fa4d4ecc-2e68-4a6e-c006-fb99bbd44824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128, 1, 1])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 2D or 3D input (got 4D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-8f99b6e7b7e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-811ce91dfdcf>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(self, number_of_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0ms_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_data\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0ms_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m                 \u001b[0ms_g_detach\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0ms_d_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_g_detach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9980d0022c67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_input_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36m_check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise ValueError('expected 2D or 3D input (got {}D input)'\n\u001b[0;32m--> 169\u001b[0;31m                              .format(input.dim()))\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected 2D or 3D input (got 4D input)"
     ]
    }
   ],
   "source": [
    "dig.model_train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZW8zOM6LOAJq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `torch.view` not found.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Digit_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
