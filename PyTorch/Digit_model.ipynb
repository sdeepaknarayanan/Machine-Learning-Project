{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4NJ2c5hiuyKS"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "#from base_test import BaseTest\n",
    "#import digits_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import data\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "#from data import NormalizeRangeTanh, UnNormalizeRangeTanh - What the hell is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1034
    },
    "colab_type": "code",
    "id": "FSwB1WEmvJyW",
    "outputId": "7b00ad82-4e1c-433e-80cd-d50f35769367"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9be7643555fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_F_SVHN_NormRange.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;31m# Not already authorized, so do the authorization dance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m   \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendcontrol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m   \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Stopped'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('model_F_SVHN_NormRange.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hm_cvLl-v8Rd"
   },
   "outputs": [],
   "source": [
    "'content/drive/My Drive/model_F_SVHN_NormRange.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rq2cFTuxEsD"
   },
   "outputs": [],
   "source": [
    "self.readClassifier('/content/drive/My Drive/model_F_SVHN_NormRange.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyyipaEFwhf_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwwe0MZuu53S"
   },
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "  \n",
    "  def __init__(self, channels, gpu=False): # What to do with GPU?\n",
    "    super(self.__class__,self).__init__()\n",
    "    self.channels=channels\n",
    "    self.gpu=gpu\n",
    "    self.network=nn.Sequential(\n",
    "                 nn.ConvTranspose2d(self.channels,512,kernel_size=(4,4), stride=1, padding=1),\n",
    "                 nn.BatchNorm1d(512),\n",
    "                 nn.ReLU(inplace=True),\n",
    "                 nn.ConvTranspose2d(512,256,kernel_size=(4,4), stride=2, padding=1),\n",
    "                 nn.BatchNorm1d(256),\n",
    "                 nn.ReLU(inplace=True),\n",
    "                 nn.ConvTranspose2d(256,128,kernel_size=(4,4), stride=2, padding=1),\n",
    "                 nn.BatchNorm1d(128),\n",
    "                 nn.ReLU(inplace=True),\n",
    "                 nn.ConvTranspose2d(128,1,kernel_size=(4,4), stride=2, padding=1),\n",
    "                 nn.Tanh()\n",
    "    )\n",
    "  def forward(self,input):\n",
    "    return self.network(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1T6YDsRIxSP4"
   },
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "  \n",
    "  def __init__(self, channels, leaky_coef=0.2):\n",
    "    super(self.__class__,self).__init__()\n",
    "    self.channels=channels\n",
    "    self.leaky_coef=leaky_coef\n",
    "    self.network=nn.Sequential(\n",
    "                 nn.Conv2d(1,128,kernel_size=(3,3), stride=2, padding=1),\n",
    "                 nn.BatchNorm2d(128),\n",
    "                 nn.LeakyReLU(self.leaky_coef,inplace=True),\n",
    "                 nn.Conv2d(128,256,kernel_size=(3,3), stride=2, padding=1),\n",
    "                 nn.BatchNorm2d(256),\n",
    "                 nn.LeakyReLU(self.leaky_coef,inplace=True),\n",
    "                 nn.Conv2d(256,512,kernel_size=(3,3), stride=2, padding=1),\n",
    "                 nn.BatchNorm2d(512),\n",
    "                 nn.LeakyReLU(self.leaky_coef,inplace=True),\n",
    "                 nn.Conv2d(512,3, kernel_size=(4,4), stride=2)\n",
    "    )\n",
    "  \n",
    "  def forward(self,input):\n",
    "    return self.network(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxatV29Sx5W9"
   },
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "\n",
    "class feature_encoder(nn.Module):\n",
    "  def __init__(self, channels, gpu=False):\n",
    "    super(feature_encoder, self).__init__()\n",
    "    self.gpu = gpu\n",
    "    self.channels = channels\n",
    "    self.classify = nn.Sequential(\n",
    "                   nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),               \n",
    "                nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(256, 128, kernel_size=4, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(4),\n",
    "                Reshape(-1,128),\n",
    "                nn.Linear(128, 10),\n",
    "                nn.Softmax(),)\n",
    "    if self.gpu:\n",
    "      self.type(torch.cuda.FloatTensor)\n",
    "      \n",
    "  def forward(self, input):\n",
    "    return self.classify(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9aW0DY2zRLW"
   },
   "outputs": [],
   "source": [
    "# def classifier(address):\n",
    "#    # Kagi will guide us\n",
    "#   pass\n",
    "\n",
    "class tanh_normalize(object):\n",
    "    ''' Normalizes a tensor with values from [0, 1] to [-1, 1]. '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        sample = sample * 2.0 - 1.0\n",
    "        return sample\n",
    "\n",
    "class digits_transfer():\n",
    "  \n",
    "  \n",
    "  def __init__(self,gpu=True):\n",
    "    \n",
    "    super(digits_transfer,self).__init__()\n",
    "    self.gpu = gpu\n",
    "    self.generator_loss_function=None\n",
    "    self.gan_loss_function=None\n",
    "    self.discriminator_loss_function=None\n",
    "    #self.source_validation_loader=None\n",
    "    self.source_test_loader=None\n",
    "    self.target_test_loader=None\n",
    "    self.distance_target_domain=None\n",
    "    self.source_train_loader=None\n",
    "    self.target_train_loader=None\n",
    "    self.batch_size=128\n",
    "    self.lossCrossEntropy=nn.CrossEntropyLoss() ## Why the fuck is it here?\n",
    "    \n",
    "  def readClassifier(self, model_name):\n",
    "\n",
    "#     old_model = torch.load(model_name)['state_dict']\n",
    "#     old_dict = old_model.load_state_dict() \n",
    "#     new_model = feature_encoder(3,self.gpu)\n",
    "#     new_dict = new_model.state_dict()\n",
    "#     new_dict = {k: v for k, v in old_dict.items() if k in new_dict}\n",
    "#     old_dict.update(new_dict) \n",
    "#     new_model.load_state_dict(new_dict)\n",
    "\n",
    "#     model = describe_model()\n",
    "#     checkpoint = torch.load('checkpoint.pth.tar')\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    model = feature_encoder(3,self.gpu)\n",
    "    temp = torch.load(model_name)\n",
    "    model.load_state_dict(temp['state_dict'])\n",
    "    self.model['f'] =model\n",
    "\n",
    "    for param in self.model['f'].parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "  def data_loader(self):\n",
    "    \n",
    "    SVHN = transforms.Compose([transforms.ToTensor(), tanh_normalize()])\n",
    "    MNIST = transforms.Compose([transforms.Scale(32), transforms.ToTensor(), tanh_normalize()])\n",
    "    \n",
    "    source_train_set = torchvision.datasets.SVHN(root='./SVHN/', split='extra', download=True, transform=SVHN)\n",
    "    self.source_train_loader = torch.utils.data.DataLoader(source_train_set, batch_size=128, shuffle=True, num_workers=8)\n",
    "    \n",
    "    source_test_set = torchvision.datasets.SVHN(root='./SVHN/', split='test', download=True, transform=SVHN)\n",
    "    self.source_test_loader = torch.utils.data.DataLoader(source_test_set, batch_size=128, shuffle=False, num_workers=8)\n",
    "    \n",
    "    target_train_set = torchvision.datasets.MNIST(root='./MNIST/', train=True, download=True, transform=MNIST)\n",
    "    self.target_train_loader = torch.utils.data.DataLoader(target_train_set, batch_size=128, shuffle=True, num_workers=8)\n",
    "    \n",
    "    target_test_set = torchvision.datasets.MNIST(root='./MNIST/', train=False, download=True, transform=MNIST)\n",
    "    self.target_test_loader = torch.utils.data.DataLoader(target_test_set, batch_size=128, shuffle=False, num_workers=8)\n",
    "  \n",
    "  def view_batch(self):\n",
    "    ## TO be filled in later\n",
    "    pass\n",
    "  \n",
    "  def make_model(self):\n",
    "    \n",
    "    self.model = {}\n",
    "    self.model['d'] = discriminator(128)\n",
    "    self.model['g'] = generator(128)\n",
    "    if self.gpu:\n",
    "      self.model['d'] = self.model['d'].cuda()\n",
    "      self.model['g'] = self.model['g'].cuda()\n",
    "    # Assumption 1 : Kagi saves everything as a Tar File\n",
    "    #self.model['f'] = classifier('Give the saved model here')\n",
    "    self.readClassifier('f_model.tar')\n",
    "\n",
    "  def create_discriminator_loss_function(self):\n",
    "    \n",
    "    def discriminator_loss(s_d_g, t_d_g, t_d):  \n",
    "      return self.lossCrossEntropy(s_d_g.squeeze(), self.label_0) + self.lossCrossEntropy(t_d_g, self.label_1) + self.lossCrossEntropy(t_d, self.label_2)\n",
    "    \n",
    "    self.discriminator_loss_function = discriminator_loss\n",
    "  \n",
    "  def create_generator_loss_function(self):\n",
    "    \n",
    "    def generator_loss(s_f, s_g_f, s_d_g, t, t_g, t_d_g, alpha, beta, gamma):\n",
    "      \n",
    "      loss_gan_generator = self.lossCrossEntropy(s_d_g.squeeze, self.label_2) + self.lossCrossEntropy(t_d_g, self.label_2)\n",
    "      loss_feature_constancy = self.mean_square_loss(s_g_f, s_f.detach())\n",
    "      loss_target_identity = self.mean_square_loss(t_g, t.detach())\n",
    "      \n",
    "      return loss_gan_generator + alpha*loss_feature_constancy + beta*loss_target_identity\n",
    "    \n",
    "    self.generator_loss_function = generator_loss\n",
    "  \n",
    "#   def create_distance_function_target_domain(self):\n",
    "#     ## If needed let's see!\n",
    "#     def distance_target(t_1, t_2):\n",
    "#       distance = self.mean_square_loss\n",
    "    \n",
    "  def make_loss_function(self):\n",
    "    self.lossCrossEntropy = nn.CrossEntropyLoss().cuda()\n",
    "    self.mean_square_loss = nn.MSELoss().cuda()\n",
    "    label_0, label_1, label_2 = (torch.LongTensor(self.batch_size) for i in range(3))\n",
    "    label_0 = Variable(label_0.cuda())\n",
    "    label_1 = Variable(label_1.cuda())\n",
    "    label_2 = Variable(label_2.cuda())\n",
    "    label_0.data.resize_(self.batch_size).fill_(0)\n",
    "    label_1.data.resize_(self.batch_size).fill_(1)\n",
    "    label_2.data.resize_(self.batch_size).fill_(2)\n",
    "    self.label_0 = label_0\n",
    "    self.label_1 = label_1\n",
    "    self.label_2 = label_2\n",
    "    \n",
    "#     self.create_distance_function_target_domain()\n",
    "    self.create_discriminator_loss_function()\n",
    "    self.create_generator_loss_function()\n",
    "  \n",
    "  def make_optimizer(self):\n",
    "    \n",
    "    generator_learning_rate = 0.001\n",
    "    generator_weight_decay = 0.000001     \n",
    "    self.generator_optimizer = optim.Adam(self.model['g'].parameters(), lr = generator_learning_rate, weight_decay = generator_weight_decay)\n",
    "    \n",
    "    discriminator_learning_rate = 0.001\n",
    "    discriminator_weight_decay = 0.000001     \n",
    "    self.discriminator_optimizer = optim.Adam(self.model['d'].parameters(), lr = discriminator_learning_rate, weight_decay = discriminator_weight_decay)\n",
    "    \n",
    "#   def validate_model(self, **kwargs):\n",
    "    \n",
    "#     gan_loss_weight = kwargs.get(\"gan_loss_weight\",0.001)\n",
    "#     validation_loss = 0\n",
    "#     self.model['g'].eval()\n",
    "#     samples = np.random.randint(0,len(source_validation_set),size = 5)\n",
    "#     for i in samples:\n",
    "# #       ## source_validation_set is a keyword argument!\n",
    "# #       source_data = source_validation_set[i]\n",
    "# #       source_generator_output = self.model['g'](source_data)\n",
    "# #       source_classifier_output = self.model['f'](source_data)\n",
    "# #       source_\n",
    "# #     \"\"\"\n",
    "# #       To be Done Later\n",
    "# #     \"\"\"\n",
    "# #     pass\n",
    "  \n",
    "  def show_image():\n",
    "    pass\n",
    "    # Fuck this shit\n",
    "  def display_results(self, source_data, source_generator):\n",
    "    \"\"\"\n",
    "      To be done later\"\"\"\n",
    "    pass\n",
    "    \n",
    "  def model_train(self, number_of_epochs, **kwargs):\n",
    "    \n",
    "    visualize_batch = kwargs.get(\"visualize_batch\",50)\n",
    "    save_batch = kwargs.get(\"save_batch\",200)\n",
    "    test_batch = kwargs.get(\"test_batch\",200)\n",
    "    \n",
    "    SVHN_count=0\n",
    "    batch_count=0\n",
    "    discriminator_run_loss=0\n",
    "    generator_run_loss=0\n",
    "    \n",
    "    l=min(len(self.source_train_loader),len(self.target_train_loader))\n",
    "    \n",
    "    for epoch in range(number_of_epochs):\n",
    "      \n",
    "      source_data_iter=iter(self.source_train_loader)\n",
    "      target_data_iter=iter(self.target_train_loader)\n",
    "      \n",
    "      for i in range(l):\n",
    "        \n",
    "        SVHN_count+=1\n",
    "        if SVHN_count>=len(self.source_train_loader):\n",
    "          SVHN_count=0\n",
    "          source_data_iter=iter(self.source_train_loader)\n",
    "          \n",
    "        source_data, source_labels=source_data_iter.next()\n",
    "        target_data, target_labels=source_data_iter.next()\n",
    "        \n",
    "        if self.batch_size != source_data.size(0) or self.batch_size != target_data.size(0):\n",
    "          continue\n",
    "          \n",
    "        batch_count+=1\n",
    "        \n",
    "        if self.gpu:\n",
    "          source_data, source_labels = Variable(source_data.float().cuda()), Variable(source_labels.float().cuda())\n",
    "          target_data, target_labels = Variable(target_data.float().cuda()), Variable(target_labels.float().cuda())\n",
    "          \n",
    "        else:\n",
    "          source_data, source_labels = Variable(source_data.float()), Variable(source_labels.float())\n",
    "          target_data, target_labels = Variable(target_data.float()), Variable(target_labels.float())\n",
    "          \n",
    "        for param in self.model['d'].parameters():\n",
    "          param.requires_grad = True\n",
    "          \n",
    "        self.model['d'].zero_grad()\n",
    "        \n",
    "        s_f = self.model['f'](source_data)      #generator \n",
    "        s_g = self.model['g'](s_f)\n",
    "        s_g_detach = s_g.detach()\n",
    "        s_d_g = self.model['d'](s_g_detach)\n",
    "        \n",
    "        t_data_3 = torch.cat((t_data,t_data,t_data),1)\n",
    "        t_f = self.model['f'](t_data_3)\n",
    "        t_g = self.model['g'](t_f)\n",
    "        t_g_detach = t_g.detach()\n",
    "        t_d_g = self.model['d'](t_g_detach)\n",
    "        t_d = self.model['d'](t_data)\n",
    "        \n",
    "        Discriminator_loss = self.discriminator_loss_function(s_d_g, t_d_g, t_d)\n",
    "        Discriminator_loss.backward()\n",
    "        self.discriminator_optimizer.step()\n",
    "        \n",
    "        for param in self.model['g'].parameters():\n",
    "          param.requires_grad = False\n",
    "          \n",
    "        self.model['g'].zero_grad()\n",
    "        \n",
    "        s_d_g = self.model['d'](s_g)\n",
    "        s_g_3 = torch.cat((s_g,s_g,s_g),1)\n",
    "        s_g_f = self.model['f'](s_g_3)\n",
    "        \n",
    "        t_d_g = self.model['d'](t_g)\n",
    "        \n",
    "        Generator_loss = self.generator_loss_function(s_f, s_g_f, s_d_g, t, t_g, t_d_g)\n",
    "        Generator_loss.backward()\n",
    "        self.generator_optimizer.step()\n",
    "        \n",
    "        discriminator_run_loss+=Discriminator_loss.data[0]\n",
    "        generator_run_loss+=Generator_loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6q-Gq_1PCNaG"
   },
   "outputs": [],
   "source": [
    "dig=digits_transfer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-V_1A3Ag7k5u"
   },
   "outputs": [],
   "source": [
    "dig.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "K5g4-2Px7_3-",
    "outputId": "8cf6e94b-2d00-40b7-a0f2-37446578b79b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:208: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./SVHN/extra_32x32.mat\n",
      "Using downloaded and verified file: ./SVHN/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "dig.data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8KSGBxxC8UOv"
   },
   "outputs": [],
   "source": [
    "dig.make_optimizer()\n",
    "dig.make_loss_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1017
    },
    "colab_type": "code",
    "id": "0-7aNN979J59",
    "outputId": "fa4d4ecc-2e68-4a6e-c006-fb99bbd44824"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-8f99b6e7b7e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-e31599d4b30e>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(self, number_of_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0ms_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_data\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0ms_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0ms_g_detach\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0ms_d_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_g_detach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-2429c1c93d2e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    756\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [128, 512, 4, 4], but got 2-dimensional input of size [128, 10] instead"
     ]
    }
   ],
   "source": [
    "dig.model_train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZW8zOM6LOAJq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Digit_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
